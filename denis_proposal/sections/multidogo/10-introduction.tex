As a dovetail between crowd-driven and expert-driven data sources, we propose an intermediate solution that pairs a person from the crowd with an expert.  
%
 This creates a verisimilitude of a customer, simulated by a worker from the crowd, interacting with a customer service agent, simulated by an actual professional customer service agent.  
%
The resulting dataset provides a stark contrast in the language generated by anonymous crowd workers and experts.\footnote{Denis Peskov,  Nancy Clarke,  Jason Krone,  Brigi Fodor,  Yi Zhang,Adel Youssef, and Mona Diab. Multi-domain goal-oriented dialogues(multidogo):  Strategies  toward  curating  and  annotating  large  scale dialogue data.  In Proceedings of  the 2019  Conference on  Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4518â€“4528, 2019.  \\
Peskov planned and implemented majority of crowd-sourcing tasks, supervised the data collection thereof, wrote part of the task guidelines,  performed data analysis, and wrote the majority of paper. 
}

\section{Introduction}


\begin{table*}[]
	\centering
	\small
	\begin{tabular}{p{.7 cm} p{7.8 cm} p{5 cm}}
		
		\textbf{Role}&  \textbf{ Turn } &  \textbf{Annotations} \\
		\hline
	
		A & Hey there! Good morning. You're connected to LMT Airways. How may I help you? & DA = \{ elicitgoal \} \\
		\rowcolor{gray!25}
		C & Hi, I wonder if you can confirm my seat assignment on my flight tomorrow? &  IC = \{ SeatAssignment \} \\
		A & Sure! I'd be glad to help you with that. May I know your last name please? &  DA = \{ elicitslot \} \\
				\rowcolor{gray!25}
		C &  My last name is Turker. & IC = \{ contentonly \}, 
		\newline SL = \{Name : Turker \} \\
		A & Alright Turker! Could you please share the booking confirmation number? & DA = \{ elicitslot \} \\
		\rowcolor{gray!25}
		C &  I believe it's AMZ685. & IC = \{ contentonly \}, 
		\newline SL = \{ Confirmation Number :  AMZ685 \} \\
		$\cdots$ & $\cdots$ & $\cdots$ \\
	\end{tabular}
	\caption{A segment of a dialogue from the airline domain annotated at the turn level.  This data is annotated with agent dialogue acts (DA), customer intent classes (IC), and slot labels (SL).  Roles C and A stand for ``Customer'' and ``Agent'', respectively.}
	\label{tab:TravelConvo}
\end{table*}



Modern Natural Language Understanding (NLU) frameworks for dialogues are by definition data hungry.  They require large amounts of training data representative of goal oriented conversations reflecting both context and diversity. But human responses in goal-oriented dialogues are less predictable than automated systems \citep{bordes2016learning}.  For example, ``Please do this'' cannot be interpreted without a broader context.  Only by seeing previous utterances, such as requests to book a flight on a specific day to a specific destination, can this task be performed.  Additionally, a single intent can be phrased in multiple ways depending on context; ``book my flight'', ``finalize my reservation'', ``Yes, the 6 pm one'' may all be referring to a flight-booking intent. Hence, entire conversations, rather than independent utterances, must be collected.  Such data is even more pertinent to modeling  \textsc{nlu} and related tasks as they require large, varied, and ideally human-generated datasets. Moreover, recent work \citep{dong2015multi,devlin2018bert} has shown the benefit of applying 
joint-training and transfer learning techniques to natural language processing tasks.
However, these approaches have yet to become widely used in dialogue tasks, due to a lack of large-scale datasets. Furthermore, the latest state of the art end-to-end neural approaches benefit from such training data even more so than past work on goal-oriented dialogues structured around slot filling~\citep{lemon2006isu,wang2013simple}.  One way to simulate data---and not risk releasing personally identifying information---for a domain is to use a Wizard-of-Oz data gathering technique, which requires that participants in a conversation fulfill a role \citep{kelley1984iterative}.  This approach has been used in popular public goal-oriented datasets: \textsc{dstc} and \multiwoz~\citep{williams2016dialog, budzianowski2018multiwoz}.

Conversations between people and automated systems occur with increasing frequency, especially in customer service.  Customers reach out to agents, which could be automated bots or real individuals, to achieve a domain-specific goal.  This creates a disparate conversation: agents are incentivized to operate within a set procedure and convey a patient and professional tone. In contrast, customers do not have this incentive. However, to date, the largest available multi-domain goal-oriented dialogue dataset assigns similar dialogue act annotations to both agents and customers \citep{budzianowski2018multiwoz}.   

To solve the aforementioned challenges, we present our efforts to curate, annotate, and evaluate a large scale multi-domain set of goal oriented dialogues.  The dataset is primarily gathered from workers in the crowd paired with professional annotators. The dataset elicited, \multidogo, comprises over 86K raw conversations of which 54,818 conversations are annotated at the turn level.  We investigate multiple levels of annotation granularity. We annotate a subset of the data on both turn and sentence levels. A turn is defined as a sequence of one or more speech/text sentences by a participant in a conversation. A sentence is a period delimited sequence of words in a turn. A turn may comprise one or more sentences. We do use the term  utterance to refer to a unit (turn or sentence, spoken or written by a participant).\footnote{ We acknowledge that the term utterance is controversial in the literature \citep{paretti18}} In our devised annotation strategy, we distinguish between dialogue speech acts for agents vs. customers. In \multidogo, the agents' speech acts   [\textsc{da}] are annotated with generic class labels common across all domains, while customer speech acts are labeled with  intent classes [\textsc{ic}]. Moreover, we annotate customer utterances with the appropriate slot labels [\textsc{sl}], which consist of the \textsc{sl} span and corresponding tokens with that \textsc{sl} tag. We present the strategies we use to curate and annotate such data given its contextual setting. We furthermore illustrate the efficacy of our devised approaches and annotation decisions against intrinsic metrics and via extrinsic evaluation, namely by applying neural baselines for \textsc{da}, \textsc{ic} and \textsc{sl} classification leveraging joint models. %% Nancy - do we annotate both customer and agent utterances with SL. I think just customer right ? 