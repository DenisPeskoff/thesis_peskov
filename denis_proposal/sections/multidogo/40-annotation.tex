
\section{Data Annotation}
We discuss the \textit{annotation} needed for our dataset.  
%
Of particular interest, a direct comparison of using experts versus the crowd is made in in Section~\ref{sec:motivation}.

\subsection{Annotated Dialogue Tasks}
Our dataset has three types of annotation: Agent dialogue acts [\textsc{da}], customer intent classes [\textsc{ic}], and slot labels [\textsc{sl}].  
%
We intentionally decouple Agent and customer speech act tags into the categories \textsc{da} and \textsc{ic}, respectively, to produce more fine-grained speech act tags than past iterations of dialog datasets. 
%
Intuitively, agent \textsc{da}s are consistent across domains and more abstract in nature, since agents have a standard form of response. 
%
On the other hand, customer \textsc{ic}s are domain-specific and can entail reserving a hotel room or ordering a burger, depending on the domain.  
%
A conversation example with annotations is provided in Table~\ref{tab:TravelConvo}.



\textbf{Agent Dialogue Acts (\textsc{da})} Agent dialogue acts are the 
most straightforward of our annotation tasks. There are eight possible \textsc{da}s in all domains: 
{\textit{ElicitGoal, ElicitSlot, ConfirmGoal, ConfirmSlot, EndGoal, Pleasantries, Other}. The names are self-explanatory.  \textit{Elicit Goal/Slot} indicates that the agent is gathering information. \textit{Confirm Goal/Slot} indicates that the agent is confirming previously provided information.  The \textit{EndGoal} and \textit{Pleasantries} tags, identify non-task related actions. \textit{Other} indicates that the selected utterance was not one of the other possible tags. Agent dialogue acts are consistent across domains and are often abstract (e.g. ElicitIntent, ConfirmSlot).
	
	\textbf{Customer Intent Classes (\textsc{ic}):}
	Unlike Agent \textsc{da}, customer \textsc{ic} vary for each domain and are more concrete.  For example, the Airline domain has a ``BookFlight'' \textsc{ic}, Fast Food has an ``OrderMeal'' \textsc{ic}, and Insurance has an ``OrderPolicy'' \textsc{ic} in our annotation schema.  Customer intents can overlap across domains (e.g. OpeningGreeting, ClosingGreeting) and other times be domain specific (e.g. RequestCreditLimitIncrease, OrderBurger, BookFlight). 
	%% Nancy why didn't we make the greetings ICs as pleasantries similar to the DA
	
	
	{\textbf{Slot Labels (\textsc{sl}):}
	Slot Labeling is a task contingent on Customer Intent Classes.  Certain intents require that additional information, namely slot values, be captured. For instance, to open a bank account, one must solicit the customer's social security number.  Slots can overlap across intents (e.g. Name, SSN Number) or they can be unique to a domain-specific intent (e.g. CarPolicy). 
	
	
	\subsection{Data Annotation Procedure}
	Our annotators use a web interface, depicted in Figure \ref{fig:SlotAnnotation}, to select the appropriate intent class for an utterance out of a list of provided options. To annotate slot labels, our annotators use their cursors to highlight slot value character spans within an utterance and then select the corresponding slot label from a list of options. The output of this slot labeling process is a list of $\langle$slot-label, slot-value, span$\rangle$ triplets for each utterance.  
	\subsection{Annotation Design Decisions}
	\label{sec:motivation}
	
	\textbf{Decoupled Agents and Customers Label Sets}
	Agents and customers have notably different goals and styles of communication.  However, past dialogue datasets do not make this distinction at speech act schema level. Specificity is important for handling unique customer requests, but a relatively formulaic approach is required of agents across different industries.  Our distinction between the customer and agent roles creates training data for a bot that explicitly simulates agents. 
	
	\textbf{Annotation Unit Granularity: Sentence vs. Turn Level}
	An important decision, which is often under discussed, is the proper semantic unit of text to annotate in a dialogue.
	Commonly, datasets provide annotations at the turn level \citep{budzianowski2018multiwoz, asri2017frames, kvret}. 
	However, turn level annotations can introduce confusion for \textsc{ic} datasets, given multiple intents may be present in different sentences of a single turn. 
	For instance, consider the turn ``I would like to book a flight to San Francisco. Also, I want to cancel a flight to Austin."
	Here, the first sentence has the BookFlight intent and the second sentence has the CancelFlight intent. 
	An turn level annotation of this utterance would yield the multi-class intent (BookFlight, CancelFlight). 
	In contrast, a sentence level annotation of this utterance identifies that the first sentence corresponds to BookFlight while the second corresponds to CancelFlight.
	We annotate a subset our data, 2,500 conversation per domain for 15,000 conversations in total, at the sentence as well as turn level to access the impact of this design choice on downstream performance. The remainder of our dataset is annotated only at the turn level. %% Mona how do we motivate the fact that we choose to annotate at the turn level for the remainder of the dataset given this drops DA classification performance and turn and sentence level annotations are comparible in experimental  for IC SL. seems like sentence level would have been the right annotation unit here. 
	
	
	\begin{table}[t!]
		\small
		\centering
		\begin{tabular}{c  c  c }
			%\multicolumn{3}{c}{\textsc{isaa}} \\
			\textbf{Dialogue Act} &\textbf{ Intent Classes}& \textbf{Slot Labels} \\
			\hline
			0.701 & 0.728 & 0.695 \\
			\hline
		\end{tabular}
		\caption{Inter Source Annotation Agreement (\textsc{isaa}) scores quantifying the agreement of crowd sourced and professional annotations.}
		\label{ipa}
	\end{table}
	
	\textbf{Professional vs. Crowd-Sourced Workers for Annotation}
	For annotation, we compare and contrast professional annotators to crowd sourced annotators on a subset of data. Professional annotators assign \textsc{da}, \textsc{ic}, and \textsc{sl} tags to the 15,000 conversations annotated at both the turn and sentence level; statistics for these conversations are given in Table \ref{table:data}. In an effort to decrease annotation cost, we employ crowd source annotators via Mechanical Turk to label an additional 54,818 conversations rated as Good or Excellent quality during data collection. We provide statistics for this set of crowd annotated data in Table \ref{MultiDoGoStats}. To compare the quality of crowd sourced annotations against professional annotations, we use both strategies to annotate a shared subset of 8,450 conversations. 
	%
	We devise an Inter Source Annotation Agreement (\textsc{isaa}) metric to quantify the agreement of these crowd sourced and professionally sourced annotations. 
	%
	\textsc{isaa} is a relaxation of Cohen $\kappa$, intended to count partial agreement of multi-tag labels. 
	%
	\textsc{isaa} defines two sets of tags, $A$ and $B$, to be in agreement if there is at least one ``shared" tag in both $A$ and $B$. $A$ and $B$ reflect the majority labels agreed upon per source (professionals or crowd workers). Using \textsc{isaa} we find that crowd sourced and professional annotations have a substantial degree of shared annotations. We report \textsc{isaa} for the \textsc{da}, \textsc{ic}, and \textsc{sl} tasks in Table \ref{ipa}. 

\begin{table*}[t!]
	\centering
	\small
	\begin{tabular}{ l  c c c c }
		\textbf{Domain} & \textbf{Elicited} & \textbf{Good/Excellent} & \textbf{IC/SL} & \textbf{DA/IC/SL}\\
		\hline
		
		Airline & 15100&14205 & 7598& 6287 \\
		Fast Food & 9639& 8674&7712& 4507 \\
		Finance & 8814& 8160& 8002& 6704 \\
		Insurance &14262 &13400 & 7799& 7434 \\
		Media & 33321& 32231& 19877& 12891  \\
		Software &5562& 4924& 3830& 2753 \\
		\hline
		\textbf{Total} &\textbf{86698} & \textbf{81594}& \textbf{54818}& \textbf{40576} \\
	\end{tabular}
	\caption{Total number of conversations per domain: raw conversations Elicited; Good/Excellent is the total number of conversations rated as such by the agent annotators; (IC/SL) is the number of conversations annotated for Intent Classes and Slot Labels only; (DA/IC/SL) is the total number of conversations annotated for Dialogue Acts, Intent Classes, and Slot Labels.} %\citet{budzianowski2018multiwoz}.  We provide counts for the training data, except for \textsc{frames}, which does not %have splits.  Our number of unique tokens and slots can be attributed to us not relying on carrier phrases.}
	%%Mona modify this caption 
	\label{MultiDoGoStats}
\end{table*}

\subsection{Quality Control}\label{QC}
We institute three processes to enforce data quality.
During data collection, our data associates report on the quality of each conversation.
Specifically, the data associates grade the conversation on a scale from ``Unusable'', ``Poor", ``Good", to ``Excellent". They were provided with guidelines to help decide on the chosen rating such as coherence, whether the dialogue achieved the purported goal, etc.   To ensure high data quality we only utilize conversations with  ``Good" or ``Excellent" ratings in subsequent annotation. 

Secondly, each conversation is annotated at least twice. 
%
We resolve inconsistent annotations by selecting the annotation given by the majority of annotators per item. 
%
We calculate inter-annotator agreement with Fleiss' $\kappa$ and find ``substantial agreement'', according to the metric.\footnote{We use Fleiss' $\kappa$ unlike in the earlier profession/crowd worker comparison as we have more than two annotators for this task.}
%
Our annotators must pass a qualification test as well as maintain an on-going level of accuracy in randomly distributed test questions throughout their annotation. 
%
Third, we pre-process our data to remove issues such as duplicate conversations and improperly entered slot value spans. We refer readers to our discussion of pre-processing in Section~\ref{Baselines} for further detail.


\begin{table*}[t!]
	\centering
	\small
	\begin{tabular}{ l  c c c c c c}
		\textbf{Bias}& \textbf{Airlines} & \textbf{Fast Food}& \textbf{Finance} &\textbf{Insurance} & \textbf{Media}& \textbf{Software} \\
		\hline
		IntentChange & & 1443 & & & & \\
		MultiIntent & 2200 & 1913 & 1799 & 1061 & 607 & 2295 \\
		MultiValue & & 354& & & & \\
		Overfill & & & 1486 & 2763 & & \\
		SlotChange & 4207 & 2011& 2506 & 3321& 570 & 2085 \\
		SlotDeletion & & 333 & & & & \\
		\hline
		\textbf{Total} &6407 &6054 & 5791&7145 &1177 & 4380\\
		\hline
	\end{tabular}
	\caption{Number of conversations per domain collected with specific biases. Fast Food had the maximum number of biases. MultiIntent and SlotChange are the most used biases.} 
	%%Mona modify this caption 
	\label{biases}
\end{table*}

\subsection{Dataset Characterization and Statistics}

\multidogo dataset is more diverse by virtue of covering more domains, but more importantly, it is more controlled since it was curated rather than being scraped from existing data sources that are not necessarily synchronous (Ubuntu).
Table \ref{MultiDoGoStats} shows the statistics for \multidogo raw conversations harvested, rated as Excellent or Good, and annotated for \textsc{da}, \textsc{ic} and \textsc{sl}.  
%
Table~\ref{biases} shows the number of conversations per domain reflecting the specific biases used.




\multidogo is several orders of magnitude larger than  comparable datasets as reflected in nearly every dimension: the number of conversations, the length of the conversation, the number of domains, and the diversity of the utterances used. Table~\ref{Comparative} illustrates a comparative statistics to existing data sets.
\begin{table*}[t!]
	\centering
	\footnotesize
	\begin{tabular}{ l c c c c c a}
		\textbf{Metric} & \textbf{\textsc{dstc}} 2 & \textbf{\textsc{woz}2.0}  & \textbf{\textsc{M2M}} &   \textbf{\textsc{MultiWOZ}} & \textbf{\textsc{MultiDoGO}}\\
		\hline
		Number of Dialogues & 1,612 & 600 & 1,500 & 8,438 & 40,576 \\ %15,665 \\ %44,365\\
		Total Number of Turns & 23,354 & 4,472 & 14,796 & 115,424 & 813,834 \\ %249,167 \\ %964,513 \\
		Total Number of Tokens & 199,431 & 50,264  & 121,977 & 1,520,970 &  9,901,235\\%2,833,807 \\ %9,647,672\\
		Avg. Turns per Dialog & 14.49 &  7.45 & 9.86 & 15.91 & 20.06\\%16.56 \\ %21.74 \\
		Avg. Tokens Per Turn & 8.54 & 11.24& 8.24 & 13.18 & 12.16\\%11.37 \\ %10.00\\
		Total Unique Tokens & 986 & 2,142   & 1,008& 24,071 & 70,003\\%34,906 \\ %103,548\\ 
		Number of Unique Slots & 8 & 4  & 14 & 25 & 73\\%80 \\ %97 \\
		Number of Slot Values & 212 & 99  & 138 & 4,510 & 55,816\\%147,949 \\ %51,676\\
		Number of Domains & 1 & 1  & 1 & 7 & 6 \\
		Number of Tasks & 1 & 1  & 2 & 2 & 3 \\
		\hline
	\end{tabular}
	%% Jason, can you update this tabble to reflect the total harvested  %% MD
	\caption{\multidogo is several times larger in nearly every dimension to the pertinent datasets as selected by \citet{budzianowski2018multiwoz}.  We provide counts for the training data, except for \textsc{frames}, which does not have splits.  Our number of unique tokens and slots can be attributed to us not relying on carrier phrases.}
	%%Mona modify this caption 
	\label{Comparative}
\end{table*}


%\section{Dataset Summary and Statistics}
\begin{table*}[h!]
	\small
	\centering
	\begin{tabular}{l r r r r r r }
		%Domain & \# of Conv & Avg Utterances & Unique Intents & Unique Slots & Conv Quality & \abr{iaa}\\  
		%Domain & \#Conv & \#Turn & \#Utterance & Unique Intents & Unique Slots & Conv Quality & \abr{iaa}\\  
		\textbf{Domain} & \textbf{\#Conv} & \textbf{\#Turn }& \textbf{\#Turn/Conv} &\textbf{ \#Sentence} & \textbf{\#Intent} & \textbf{\#Slot} \\  
		\hline
		% without conv quality
		Airline & 2,500 & 39,616 & 15.8 (15) & 66,368 & 11 & 15    \\
		Fast Food & 2,500 & 46,246 & 18.5 (18) & 73,305 & 14 & 10   \\
		Finance & 2,500 & 46,001 &  18.4 (18) & 70,828 & 18 & 15    \\
		Insurance & 2,500 & 41,220 & 16.5 (16) & 67,657 & 10 & 9  \\
		Media & 2,500 & 35,291 & 14.1 (14) & 65,029 & 16 & 16     \\
		Software & 2,500 & 40,093 & 16.0 (15) & 70,268 & 16 & 15    \\
		
			\hline
	\end{tabular}
\caption{Data statistics by domain. Conversation length is shown in \textit{average (median)} number of turns per conversation. Inter-annotator agreement (\abr{iaa}) is measured with Fleiss' $\kappa$ for the three annotation tasks: Agent \abr{da} (\abr{da}), Customer \abr{ic} (\abr{ic}), and Slot Labeling (\abr{sl}).}
\label{table:data}
\end{table*}

\begin{table*}[h!]
	\small
	\centering
\begin{tabular}{l r r }
Domain & Turn-level \abr{iaa} & Sentence-level \abr{iaa} \\
\hline
Airline  & 0.514/0.808/0.802 & 0.670/0.788/0.771 \\
Fast Food  &  0.314/0.700/0.624 & 0.598/0.725/0.607 \\
Finance  &  0.521/0.827/0.772 & 0.700/0.735/0.714 \\
Insurance  &  0.521/0.862/0.848 & 0.703/0.821/0.826  \\
Media &  0.499/0.812/0.725 & 0.678/0.802/0.758 \\
Software &  0.508/0.748/0.745 & 0.709/0.764/0.698\\
	
			\hline
\end{tabular}
\caption{ Inter-annotator agreement (\abr{iaa}) is measured with Fleiss' $\kappa$ for the three annotation tasks: Agent DA (DA), Customer IC (IC), and Slot Labeling (SL).}	

\label{table:data}
\end{table*}
	
We provide summary statistics for the subset of our data annotated at both turn and sentence granularity in Table~\ref{table:data}.  This describes the total size of the data  per domain in number of conversations, turns, the unique number of intents and slots, and inter-annotator agreement (\abr{iaa}) for both turn and sentence level annotations. It is worth observing that the \textsc{da} annotations achieve a much higher \abr{iaa} in Sentence level annotations compared to Turn level annotation, most notably in the Fast Food domain. \textsc{ic} and \textsc{sl} annotations reflect a slightly higher \abr{iaa} in Turn level annotation granularity compared to Sentence level.  

	
\begin{figure}[t!]
	% By Jason: Replaced image with text box to make it easier to read the instructions
	%\includegraphics[scale=.44]{emnlp_2019/images/example_conversation.pdf}
	\centering
	\textbf{Agent Instructions}\par\medskip
	\small
	\fbox{\begin{minipage}{35em}
			Imagine you work at a bank.
			Customers may contact you about the following set of issues:
			checking account balances (checking or savings),
			transferring money between accounts, and
			closing accounts.\\
			
			\textbf{GOAL}: Answer the customer's question(s) and complete their request(s). \\
			
			For any request, you will need to collect at least the following information to be able to identify the customer: name, account PIN *or* last 4 digits of SSN. \\
			
			For giving information on balances, or for closing accounts, you will also need the last 4 digits of the account number. \\
			
			For transferring money, you will also need: last 4 digits of account to move from, last 4 digits of account to move to, and the sum of money to be transferred. \\
			
			Your customer may ask you to do only one thing; that's okay, but make sure you confirm you achieved everything the Customer wanted before completing the conversation.
			Don't forget to signal the end of the conversation (see General guidelines)
	\end{minipage}}
	\caption{Agents are provided with explicit fulfillment instructions. These are quick-reference instructions for the Finance domain. Agents serve as one level of quality control by evaluating a conversation between Excellent and Unusable.}  %Additional reference material is provided, but not pictured.}
	\label{Instructions}
\end{figure}

	\begin{table*}[t!]
	\footnotesize
	\centering
	\begin{tabular}{ c c  c c c  c c c  c c c }
		
		& & \multicolumn{3}{c}{Airline} & \multicolumn{3}{c}{Fast Food} & \multicolumn{3}{c}{Finance}   \\
		\hline
		Model & Annot & DA & IC & SL & DA & IC & SL & DA & IC & SL \\
		\hline
		MFC & S & 60.57 & 33.69 &	38.71 &	57.14 &	25.42 &	61.92 &	51.73 &	37.37 &	34.07 \\
		\lstm & S & 97.20 & 90.84 &	74.16 &	90.40 &	86.09 &	72.93 &	93.90 &	90.06 &	69.09 \\
		\elmo & S & \cellcolor{red!25}\textbf{97.32} & \cellcolor{red!25}\textbf{91.88} & \cellcolor{red!25}\textbf{86.55} & \cellcolor{red!25}\textbf{91.03} & \textbf{87.95} & \textbf{77.51} & \cellcolor{red!25}\textbf{94.07} & \textbf{91.15} & \textbf{77.36} \\
		\hline
		MFC & T & 33.04 & 32.79 &	37.73 &	33.07 &	25.33 &	61.84 &	36.52 &	38.16 &	34.31 \\
		\lstm & T & \textbf{84.25} & 89.15 &	75.78 &	\textbf{66.41} &	87.35 &	73.57 &	76.19 &	92.30 &	70.92 \\
		\elmo & T & 84.04 & \textbf{89.99} &	\textbf{85.64} &	65.69 &	\cellcolor{red!25}\textbf{88.96} &	\cellcolor{red!25}\textbf{79.63} &	\textbf{76.29} &	\cellcolor{red!25}\textbf{94.50} &	\cellcolor{red!25}\textbf{79.47} \\
		\hline
		& & \multicolumn{3}{c}{Insurance} & \multicolumn{3}{c}{Media} & \multicolumn{3}{c}{Software} \\
		\hline
		Model & Annot & DA & IC & SL & DA & IC & SL & DA & IC & SL \\
		\hline
		MFC & S & 56.87 &	38.37 &	53.75 &	57.02 &	30.42 &	82.06 &	58.14 &	33.32 &	53.96 \\
		\lstm & S & \cellcolor{red!25}\textbf{94.73} &	93.30 &	75.27 &	\cellcolor{red!25}\textbf{94.27} &	92.35 &	90.84 &	93.22 &	90.95 &	69.48 \\
		\elmo & S & 94.63 &	\textbf{94.27} &	\textbf{88.45} &	\cellcolor{red!25}\textbf{94.27} &	\textbf{93.32} &	\cellcolor{red!25}\textbf{93.99} &	\cellcolor{red!25}\textbf{93.66} &	\cellcolor{red!25}\textbf{92.25} &	\textbf{76.04} \\
		\hline       
		MFC & T & 36.39 &	39.42 &	54.66 &	29.90 &	31.82 &	78.83 &	36.79 &	33.78 &	54.84 \\
		\lstm & T & \textbf{75.37} &	94.75 &	76.84 &	\textbf{77.94} &	94.35 &	87.33 &	\textbf{83.32} &	89.78 &	72.34 \\
		\elmo & T & 75.34 &	\cellcolor{red!25}\textbf{95.39} &	\cellcolor{red!25}\textbf{89.51} &	77.81 &	\cellcolor{red!25}\textbf{94.76} &	\textbf{91.48} &	82.97 &	\textbf{90.85} &	\cellcolor{red!25}\textbf{76.48} \\
		\hline 
	\end{tabular}
	\caption{Dialogue act (\abr{da}), Intent class (\abr{ic}), and slot labeling (\abr{sl}) F1 scores by domain for the  majority class, \lstm, and \elmo baselines on data annotated at the sentence (S) and turn (T) level. Bold text denotes the model architecture with the best performance for a given annotation granularity, i.e. sentence or turn level. Red highlight denotes the model with the best performance on a given task across annotation granularities.}
	\label{icslresults}
\end{table*}


\begin{table*}[t!]
	\centering
	\scriptsize
	\begin{tabular}[\linewidth]{ c  c c  c c  c c  c c  c c  c c }
		& \multicolumn{2}{c}{Airline} & \multicolumn{2}{c}{Fast Food} & \multicolumn{2}{c}{Finance} & \multicolumn{2}{c}{Insurance} & \multicolumn{2}{c}{Media} & \multicolumn{2}{c}{Software} \\
		\hline
		A & Single & Joint & Single & Joint & Single & Joint & Single & Joint & Single & Joint & Single & Joint \\
		\hline
		S & 97.32 &	\cellcolor{red!25}\textbf{97.44} &	91.03 &	\cellcolor{red!25}\textbf{91.26} &	94.07 &	\cellcolor{red!25}\textbf{94.27} &	94.63 &	\cellcolor{red!25}\textbf{94.99} &	94.27 &	\cellcolor{red!25}\textbf{94.47} &	93.66 &	\cellcolor{red!25}\textbf{94.00} \\
		T & 84.04 &	\textbf{84.64} &	\textbf{65.69} &	65.35 &	\textbf{76.29} &	75.68 &	75.34 &	\textbf{75.89} &	77.81 &	\textbf{78.56} &	82.97 &	\textbf{83.76} \\
		\hline 
	\end{tabular}
	\caption{Joint training of ELMo on all agent DA data leads to a slight increase in test performance.  However, we expect stronger joint models that leverage transfer learning should see a larger improvement.  Bold text denotes the training strategy, i.e. single domain (Base) or multi-domain (Joint), with the best performance for a given annotation granularity. Red highlight denotes the strategy with the highest DA F1 score across annotation granularities.}
	\label{jointresults}
\end{table*}