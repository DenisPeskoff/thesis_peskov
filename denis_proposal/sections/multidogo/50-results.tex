\section{Dialogue Classification Baselines}\label{Baselines}
To establish baseline performance for the \multidogo dataset we pre-process, create dataset splits, and evaluate the performance of three baseline models for each domain. 

\textbf{Pre-processing:}
We pre-process the corpus of dialogues for each domain to remove duplicate conversations and utterances with inconsistent annotations. 
The most common source of inconsistent annotations in our dataset is imprecise selection of slot label spans by annotators, which results in sub-token slot labels. 
While much of this inconsistent data could likely be recovered by mapping each character span to the nearest token span, we drop these utterances to ensure these errors have no effect on our experimental results.
Our post-processed data is pruned to approximately 90\% of the original size. 
%\paragraph{Split Formation:}
We form splits for each domain at the conversation level by randomly assigning
70\% of conversations to train, 10\% to development, and 20\% to test.
Conversation level splits enable the application of contextual models to our dataset, as each conversation is assigned to a single split. However, our conversation level splits result in imbalanced intent and slot label distributions. %We considered balancing classes in the test set, but decide that maintaining a full conversation is integral to the task.  Hence certain intents, slots, and acts occur more than others.

\textbf{Models:}
We evaluate the performance of two neural models on each domain. The first is a bi-directional \lstm{}~\citep{hochreiter1997long} with GloVe word embeddings, a hidden state of size 512, and two fully connected output layers for slot labels and intent classes respectively. The second model, \elmo{}, is similar to the \lstm{}  architecture but it additionally uses pre-trained \elmo{}~\citep{Peters:2018} embeddings in addition to GloVe word embeddings, which are kept frozen during training.  We combine these \elmo{} and GloVe embeddings via concatenation.
%For brevity, we refer to the first model as \lstm and the second model as \elmo. 
As a sanity check, we also include a most frequent class (\abr{mfc}) baseline. The \abr{mfc} baseline assigns the most frequent class label in the training split to every utterance $u'$ in the test split for both \textsc{da} and \textsc{ic} tasks. To adapt the \abr{mfc} baseline to \textsc{sl}, we compute the most frequent slot label \abr{mfc}$(w)$ for each word type $w$ in the training set. Then given a test utterance $u'$, we assign the pre-computed, most frequent slot \abr{mfc}($w'$) to each word $w' \in u'$ if $w'$ is present in the training set. If a given word $w' \in u'$ is not present the training set, we assign the \textit{other} slot label, which denotes the absence of a slot, to $w'$. 
We use the AllenNLP \citep{Gardner2017AllenNLP} library to implement these models and evaluate our performance. 
%To access the impact of including previous turns as context, we experiment with a contextual version of the \lstm. This contextual \lstm encodes the tokens for the previous n-turns into a final hidden state that is concatenated with the hidden representations for the current turn at every time-step. 
%\subsection{Hyper-parameters}
We use the Adam optimizer \citep{kingma2014adam} with a learning rate of 0.001 to train the \lstm{} and \elmo{} models for 50 epochs, using batch sizes 256 and 128, respectively.  In addition, we employ early stopping on the validation loss with a tolerance of 10 epochs to prevent over fitting.

\textbf{Evaluation Metrics:}
We report micro F1 score to evaluate \textsc{da} and \textsc{ic} performance of our models. Similarly, we use a span based F1 score, implemented in the seqeval\footnote{https://github.com/chakki-works/seqeval} library,  to evaluate SL performance.
%This span based F1 score is more strict than a per token F1 score given it requires that start and end locations of the corresponding slot span match the ground truth annotations for a prediction to be considered correct. 

\subsection{Results}
\textbf{\abr{da}/\abr{ic}/\abr{sl} Results.} Table \ref{icslresults} presents the \abr{mfc}, \lstm{}, and \elmo{} results for each domain, on the subset of 15,000 conversations annotated at both the turn and sentence levels. In general for both granularities Turn and Sentence, both \lstm{}, and \elmo{} outperform \abr{mfc} significantly across all domains.  %One point of note for these results is the relatively high \textsc{da} performance of the majority class baseline. This performance is due to the large degree of imbalance with respect to dialogue acts in the dataset. In short, this metric should be interpreted literally as the proportion of the test split in each domain made up by the most common \textsc{da}. 
Relative to the \lstm, we find that \elmo{} obtains a modest increase in \textsc{ic} accuracy of 0.41 to 2.20 F1 points and a significant increase in \textsc{sl} F1 score on all domains. Concretely, \elmo{} boosts \textsc{sl} F1 performance by 3.16 to 13.17 F1 points. We see the biggest \textsc{sl} gains on the Insurance domain, where sentence level \elmo{} achieves the 13.17 point F1 gain and turn level \elmo{} achieves a 12.67 point F1 gain. Performance gains on the Airline domain are also large; here, \elmo{} increases sentence and turn level \textsc{sl} F1 score by 12.38 and 9.86 F1 points, respectively. 
%% Jason; add discussion of specific sl gains
Both \lstm{} and \elmo{} yield similar performance in terms of F1 score on \textsc{da} classification for which the difference in performance of these models is within one F1 point across all domains. In general, the Fast Food domain yields the overall lowest absolute F1 scores. Recall that Fast Food had the most diverse dialogues (biases) as per Table \ref{biases} and the lowest \abr{iaa} as per Table \ref{table:data}.
%n general we note that the results are in synch with the IAA. We note that there is 

\textbf{Sentence vs. Turn Level Annotation Units.}
Regarding the performance of the \lstm{} and \elmo{} models on sentence vs. turn level annotation units, our results suggest that turn level annotations increase the difficulty of the \textsc{da} classification task. This finding is evidenced by \textsc{da} performance of our models on the Fast Food domain, for which F1 score is up to 25 F1 points lower for turn level annotations than sentence level annotations. We believe the increased difficulty of turn level \textsc{da} relative to sentence level \textsc{da} is driven by a corresponding increase in the confusability of turn level dialogue acts. This assertion of greater turn level \textsc{da} confusability is supported by the lower inter annotator agreement (\abr{iaa}) scores on turn level \textsc{da}, which range from 0.314 to 0.521, relative to \abr{iaa} scores for sentence level \textsc{da}, which range from 0.598 to 0.709.  This experimental result highlights the importance of collecting sentence level annotations for conversational \textsc{da} datasets. Somewhat surprisingly, our models achieve similar \textsc{ic} F1 and \textsc{sl} F1 scores on turn and sentence level annotations. We hypothesize that the choice of annotation unit has a lesser impact on the \textsc{ic} and \textsc{sl} tasks because customer utterances are more likely to focus on a single speech act, whereas Agent utterances may be more complex in comparison and include a greater number of speech acts. 

	\textbf{Joint Training on Agent DA.} Agent \textsc{da} classification naturally lends itself to joint training, given agent \textsc{da}s are shared among all domains. To explore the benefits of multi-domain training, we jointly train an agent \textsc{da} classification model on all domains and report test results for each domain separately. These results are provided in Table~\ref{jointresults}. This straightforward technique leads to a consistent but less than one point improvement in F1 scores. We expect that more sophisticated transfer learning methods \citep{liu2017adversarial, howard2018universal} could generate larger improvements for these domains. 

Overall, our results demonstrate that there is still headroom for performance improvement, especially for the \textsc{sl} task, across all domains. Consequently, \multidogo should be a relevant benchmark for developing new state-of-the-art \abr{nlu} models for the foreseeable future.  
