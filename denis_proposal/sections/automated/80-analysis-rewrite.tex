
\section{Dataset and Model Analysis}

\label{sec:analysis}

We analyze our discuss our datasets with automatic metrics. %after validating the reliability of our data (Section~\ref{sec:data}).
%
We compare our dataset to the original \abr{\quac} questions and to automatically generated questions by our models. 
%
Then, we manually inspect the sources of rewriting errors in the seq2seq baseline.
%
Further improvements for the \asr{} dataset and \name{} are possible.  

\subsection{Anaphora Resolution and Coreference}
%
Our rewrites are longer, contain more nouns and less pronouns, and have more  word types than the original data. 
%
Machine output lies in between the two human-generated corpora, but quality is difficult to assess. 
%
Figure~\ref{fig:bargraph} shows these statistics.  
%
We motivate our rewrites by exploring linguistic properties of our
data. 
%
Anaphora resolution and coreference are two core \textsc{nlp} tasks applicable to this dataset.%, in addition to the downstream tasks evaluated in Section~\ref{sec:expr}.  
%%DENISTODO
%\jbgcomment{avoid ``perform''.}


%QuAC provides many open ended questions.  20\% below human F1 level.  Long Tokens is a selling point.  Issue is context (CITE QuAC)


\begin{figure}[t!]
  \centering
	\includegraphics[width=.7\linewidth]{\figfile{length_and_ratio.pdf}}
	\caption{Human rewrites are longer, have fewer pronouns, and
          have more proper nouns than the original \abr{\quac} questions.
%
Rewrites are longer and contain more proper nouns than our Pronoun Sub
baseline and trained Seq2Seq model.}
	\label{fig:bargraph}
\end{figure}

%%DENISTODO
%\jbgcomment{This paragraph is very clunky.  As you go through, explain
 % why these numbers are they way they are.  E.g., ``\quac depends on
 % context; 54\% of \quac questions have a pronoun.''  Don't just give
  %numbers, give the {\bf why} you're mentioning these things.}

Pronouns occur in 53.9\% of \abr{\quac} questions.  Questions with pronouns are more likely to be ambiguous than those without any.   
%
 Only 0.9\% of these have pronouns that span more than one category
 (e.g., `she' and `his').  Hence, pronouns within a single sentence are likely unambiguous.  
%
 However, 75.0\% of the aggregate history has pronouns and the percentage of mixed category pronouns increase to 27.8\% of our data.  Therefore, pronoun disambiguation potentially becomes a problem for a quarter of the original data.  An example is provided in Table~\ref{tab:coreferenceexample}.
%

%
%Our automated baselines that replace pronoun create less proper
%nouns than human rewrites.
%
%Our task has over 10,000 questions that are likely to be
%interesting candidates for anaphora resolution.
%

 
\begin{table}
	\small
	\centering
	\begin{tabular*}{\linewidth}{l p{10cm}}

		{\bf Label} & {\bf Text} \\
          \hline
        
         QUESTION & How long did he stay there? \\
         \rowcolor{gray!25}
		REWRITE  & How long did Cito Gaston stay at the Jays? \\
		
		HISTORY & \parbox{10 cm}{\textit{Cito Gaston}
                           \newline  {\bf Q:} What did Gaston do after the world series? \dots \newline {\bf Q:} Where did he go in 2001? 
                           \newline {\bf A:} In 2002, he was hired by the Jays as special assistant to president and chief executive officer Paul Godfrey.} \\
	\end{tabular*}
	\caption{An example that had over ten flagged proper nouns in the history. Rewriting requires resolving challenging coreferences.}
	  
	\label{tab:coreferenceexample}
\end{table}

% \subsection{Types of Rewrites}

%I did my best to do that, spent a lot of time
%reading about different types of ellipsis
%and the other phenomena, but did not find enough
%resources to learn how those phenomna apply
%to dialogue. Did not take the risk of writing about
%something I do not understand. QuAC authors
%did not even cite any papers on that
%and the reviewers did not ask for it. yes, good to have, but 
%I do not know enough to do it.
% \jbgcomment{Still needs LINGUISTIC MOTIVATION on pronoun resolution
%  and context: what linguistic properties does this address (anaphora,
%  implicature, pragmatics, deictic expressions), and give
%  examples/citations}

Approximately one-third of the questions generated by our
pronoun-replacement baseline are within 85\% string similarity to our
rewritten questions.
%
That leaves two-thirds of our data that cannot be solved with pronoun resolution alone. 
%
%We notice several categories of rewrites: pronoun disambiguation, recipient specification, and question elucidation. 
%
%Our edits also look at coreference resolution.
%%DENISTODO
%\jbgcomment{``look at'' is vague, make clearer: why do you do this, how does this contrast with simple pronoun counting?}

\begin{comment}

\subsection{Model Analysis}
\label{sec:models}
\begin{table*}
\centering
\begin{tabular}{lp{6cm}p{8.5cm}}
	  \toprule
	 & \textbf{Seq2Seq output } & \textbf{Reference}\\
  \hline
1 & What did Chamberlain's men do? & What did Chamberlain's men do during the Battle of Gettysburg? \\
  %\hline
2 & How many games did Ozzie Smith win? & How many games did the Cardinals win while Ozzie Smith played? \\
  %\hline
3 & Did 108th get to the finals? & Did the US Women's Soccer Team get to the finals in the 1999 World Cup? \\
  %\hline
4 & Did Gabriel Batistuta reside in any other countries, besides touring in the Copa America? & Besides Argentina, did Gabriel Batistuta reside in any other countries? \\
%  \hline
5 & Did La Comedia have any more works than La Comedia 3? & Did Giannina Braschi have any more works than United States of Banana, La Comedia and Asalto al tiempo? \\
\bottomrule
\end{tabular}
\caption{Example erroneous rewrites generated by the Seq2Seq models
and their corresponding reference rewrites. The dominant
source of error is the model tendency to produce
short rewrites (Examples 1--3). Related entities (Copa America and Argentina in Example 4) distract the model. The model
struggles with listing multiple entities mentioned in different
parts of the context (Example 5).
}
  \label{tab:erranalysis}
\end{table*}

%\jbgcomment{Table / Example should be capitalized here}

By manually examining the predictions of the seq2seq model, we notice that the main source of errors is that the model tends to find a short path to completing the rewrites. That often results in \textit{under-specified questions} as in Example~1 in Table~\ref{tab:erranalysis}, \textit{question meaning change} as in Example~2 or \textit{meaningless questions} as in Example~3.

Another source of errors is having related entities mentioned in the context as 
Example~4 in Table~\ref{tab:erranalysis}, where the model confused ``Copa America''
with ``Argentina''. The model also struggles with listing multiple entities mentioned in different parts of the context. Example~5 in Table~\ref{tab:erranalysis}
show the output and the reference rewrites of the question \textit{``Did she have any more works than those 3?''}, where two of the three
	entities---``United States of Banana'', ``La Comedia'' and ``Asalto al tiempo''---are lost in the rewrite.  

\end{comment}


\subsection{Confidence in Data Quality}
\label{sec:discussion}


Confidences are a readily human-interpretable concept that may help build trust in the output of a system.
%
Transparency in the quality of up-stream content can lead to downstream improvements in a plethora of \textsc{nlp} tasks.

Exploring sequence models or alternate data representations may lead
to further improvement.  Including full lattices may mirror past results for machine
translation~\citep{sperber17emnlp} for the task of question answering.
%Phone-level approaches work in Chinese~\citep{lee2018odsqa}, but our phone
%models had lower accuracies than the baseline, perhaps due to a lack
%of contextual representation.
Using unsupervised approaches for \asr{}
\citep{wessel2004unsupervised,lee2009unsupervised} and training \asr{}
models for decoding \qb{} or Jeopardy! words are avenues for further
exploration.

\subsection{Can Question Answering Audio be Automated? }
\label{sec:conclusion}

Question answering, like many \textsc{nlp} tasks are impaired by noisy inputs.
%
Introducing \asr{} into a \abr{qa} pipeline corrupts the data. 
%
A neural model that uses the \asr{} system's confidence outputs and systematic forced decoding of words rather than unknowns improves  \textsc{qa} accuracy on \qb{} and Jeopardy! questions.  
%
Our methods are task agnostic and can be applied to other supervised \textsc{nlp} tasks.
%
Larger \textit{human-recorded} question datasets and alternate model approaches
would ensure spoken questions are answered accurately, allowing human
and computer trivia players to compete on an equal playing field.
%
Text-to-Speech technology can create a large dataset, but the unvarying pronunciation, speed, and voice---every single \abr{tts} voice is female---ultimately inhibits this approach from being a gold-standard.  