% intro
\chapter{The Case for Upfront Investment in Data}
\label{sec:intro}

Computation can solve tasks across multiple areas of scientific inquiry: natural language processing, computer vision, biology, etc.  
%
Solving tasks for all these domains---translating a sentence between languages, distinguishing a cat and a dog, classifying a mutation---has two abstract and intertwined dependencies: model-building and data collection.\footnote{~\citet{mitchell1997introduction} defines a machine learning model as,``A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E''.  E depends on data collection.}
%
The relationship is intertwined since today's models are optimized to draw statistical conclusions from significant amounts of data through machine learning.  
%
But, even the most cutting edge modeling techniques are heavily dependent on having \textit{realistic} and \textit{accurate} data for solving a task.
%
Large datasets are primarily gathered from existing online repositories or created through low-cost crowd-sourcing~\citep{deng2009imagenet,rajpurkar-16,budzianowski2018multiwoz}.
%
We argue that a new paradigm of high-quality, expert-reliant data collection can lead to long-term improvements in Natural Language Processing (\nlp{}).


\section{Where do Data come from?}

In the overview, we discuss the two tasks necessary for data collection and explain the importance of data quality for computer science as a field.  

Data creation can be broadly categorized into two categories: \textit{generation} and \textit{annotation}.  
%
We define \textit{generation} as the creation of a data item that is not previously available (e.g., sequencing a genome, creating a new image, gathering a new sentence from a user, or automatically creating a sentence)~\citep{atkins1992corpus,goodfellow2014generative,zhu2018texygen}.
%
We define \textit{annotation} as the application of a label to an existing data item (e.g., classifying a part of the genome, labeling an image as a cat, or describing the sentiment of a sentence)~\citep{deng2009imagenet,Finin2010AnnotatingNE,kozomara2014mirbase}.
%
In many fields, data must be both \textit{generated} to be representative of the task and then accurately \textit{annotated} to be effective.

The demand of neural models for quantity has caused models to be trained on large, noisy data~\citep{brown2020language}.  
%
The building blocks of other research areas---gene sequences in biology and individual pixels in computer vision---are not readily human interpretable by default.  
%
In more human-intuitive fields like natural language processing, data have reached the scale where its veracity---the certainty and completeness of the data---cannot be assumed~\citep{qiu2016survey}.
%
As a result, reformatting and preprocessing the data is necessary for training models~\citep{Hinton2006ReducingTD}.
%

\citet{atkins1992corpus} posit that,``there is in fact little danger of obfuscation for the major parameters that characterize a corpus: its size (in numbers of running words), and gross characterizations of its content.''
%
However, the objectivity of size is questionable; a corpus consisting of the same word repeated a million times clearly differs from one with a million unique words.  
%
They crucially comment that the evaluation of corpora has not been standardized.  
%
This focus on size above quality has shaped data creation during the past decade.  
%
But, the sheer quantity of data can mask biases and artifacts, as they are no longer obvious to the naked human eye~\citep{Pruim2015ICAAROMAAR, Gururangan2018AnnotationAI}.  
%
Since current approaches to machine learning often obscure how decisions are made by a model, the quality of the data is likely neither carefully evaluated by human nor machine.  

The current paradigm of crowd-sourcing---``obtaining needed services, ideas, or content by soliciting contributions from a large group of people and especially from the online community rather than from traditional employees or suppliers''~\citep{mw:crowd}---for dataset creation has been the main impetus of unreliability in data.
%
Specifically, natural language processing has generally depended on low-cost crowds following the popularity of ImageNet~\citep{deng2009imagenet}.  
%
However, the entirely crowd-sourced annotations still have notable problems after a decade of updates~\citep{yang2020towards} and should serve as a cautionary tale. 
%
A re-prioritization to working with users that have a reputational incentive to generate realistic and reliable data is a solution to this problem. 

We argue that investing in reliable data upfront, using experts, can address quality control issues and lead to further model accuracy.   
%
This improvement in the quality and diversity of data  is a prudent long-term investment as high-quality datasets can have shelf-lives of decades~\citep{marcus1993building, Miller95wordnet} while model architectures are frequently supplanted. %~\citep{vapnik2013nature, kim2014convolutional}.  
%
Additionally, experts can enable tasks in computer science not otherwise possible; generalists cannot annotate medical images and generalists that do not speak a given language cannot generate believable sentences.  


%and can benefit from a re-prioritization to investing in reliable data upfront from a paradigm of ex-post-facto quality control.

\section{Natural Language Processing}

We focus on specifically Natural Language Processing (\nlp{}) since computer science is too broad of a field to cover.  
%
We introduce the \nlp{} tasks covered in our work, challenges faced in \nlp{} due to data quality, and the various methods of data collection which impact this quality.  

A large focus of \nlp{} is on building models that exploit patterns in language data to solve a variety of tasks: question answering, conversational agents, machine translation, information extraction, etc.  
%
However, in the current paradigm of machine learning, models answer questions or make translations based on existing training data.
%
This makes \textit{robust} and \textit{natural} data a prerequisite for any meaningful model.  

The increasing dependence on neural models has exacerbated the focus on dataset size. 
%
Chapter~\ref{ch:background} describes the history of data collection in \nlp{} and explains why this dependence has grown over time.  
%
At the extreme end, GPT-3 is trained on 499 \textit{billion} tokens, which is the closest anybody has come to training a model based on the entire Internet~\citep{brown2020language}.  
%
However, not everything on the Internet is relevant or accurate.  
%
Training data containing low-quality data unsurprisingly leads to models learning controversial or false conclusions, with high levels of confidence~\citep{wolf2017we, wallace2019universal}. 
%

Additionally, many tasks in \nlp{} depend on accurate annotation.  
%
As a thought experiment, if all verbs are labeled as nouns and all nouns are labeled as verbs in the training data, a perfectly designed language model would be confidently wrong in its predictions.  
%
Crowd-sourcing with generalists~\citep{buhrmester2016amazon} assumes that enough unspecialized workers will answer a question correctly.  
%
This is a valid assumption for unambiguous, multiple-choice annotation with a large amount pool of annotators.  
%
However, many tasks require language \textit{generation}, which cannot be easily verified through \iaa{}.  
%
As a result, the \nlp{} corpora for a given task may not be reflective of the \textit{actual} task.
%
This motivates high-quality \textit{generation} and \textit{annotation} for \nlp{}.
 
We propose an expert-driven paradigm for collecting \nlp{} corpora.  
%
First, we show limitations of using unspecialized and automated methods of data collection in Chapter~\ref{ch:unspecialized}.
%
Second, we discuss hybrid approaches---using verified experts paired with external, low-cost data sources~\citep{vukovic2010towards}---in Chapter~\ref{ch:hybrid}.
%
Third, we describe an expert-designed experiment on evaluating coreference translation between German-English in Chapter~\ref{ch:contracat}.  This type of work is impossible without collaboration between native speakers in both languages and indirectly evaluates the quality of training data.    
%
Fourth, we describe an experiment on deception involving the board game of Diplomacy and experts from the community in Chapter~\ref{ch:expert}; this project represents a task that could not be meaningful without the use of experienced board game players willing to dedicate a continuous month.  
%
%We propose an additional project in Chapter~\ref{ch:proposal} that can only be verified with an expert-defined gold standard.  



\section{Proposal}

Our past work establishes that the quality of datasets can vary significantly based on who creates the data: experts or generalists.  
%
Chapter~\ref{ch:proposal} proposes to extend the use of experts to another subfield of \nlp{}: machine translation, which will benefit from increased scrutiny of data quality.    
%
This subfield now relies on a crowd-sourcing paradigm for both generation and annotation~\citep{cer2017semeval, tydiqa}.
%
We propose cultural adaptation, a new complicated task that requires cultural experts for evaluation.  
%
% discusses the proposed work needed to assess the quality difference between low-cost and high-cost data collection approaches and introduces a proposed method that applies human-verified data to machine translation in an automatic fashion.  

\begin{comment}
First, we propose a new dataset that evaluates a machine translation model's ability to \textit{understand} coreference.  
%
\citet{ribeiro2020beyond}'s CheckList  challenges the assumption that quantitative accuracy is enough for model evaluation.  
%
Our work will extend this approach for machine translation and will serve as a way to compare models build on varying of data types; low-quality models will likely not have the reasoning necessary to predict synonyms and natural language variations that those created with experts would.  
%
The seemingly straightforward translation of 
the English pronoun \textit{it} into German requires knowledge at the syntactic, discourse and world knowledge levels for proper pronoun coreference resolution (\coref{}).
% A German pronoun 
The German third person pronoun can have three genders, determined by its antecedent: masculine (\emph{er}), feminine (\emph{sie}) and neuter (\emph{es}).
%
Despite promising results for pronoun translation
\citep{bawden-etal-2018-evaluating,mueller2018,lopes2020document}, the question remains: Are neural models truly \textit{learning} this task, or are they exploiting simple heuristics to make a coreference prediction?
%
We will collaboratively build the dataset to answer this in question with American and German native speakers.  
Removed from proposal 
\end{comment}

%Second, we propose a new \nlp{} task that is only possible to verify with experts: cultural adaptation.  
%new method that approximates expert-level quality at scale for machine translation.  
%
%Is there a way to have the cost of crowd-sourcing but the quality of experts?
%
A challenge for modern data-hungry natural language processing
(\abr{nlp}) techniques is to replicate the impressive results for
standard English tasks and datasets to other languages.
%
Literally translating text into the target language is the most obvious solution.  
%
This can be the best option for tasks such as sentiment
analysis~\citep{araujo-16}, but for other tasks such as question
answering, literal translations might miss cultural nuance
if you directly translate questions from English to German to provide
additional training data.
%
While this might allow question answering systems to answer questions about
baseball and \textit{Tom Hanks} in German, it does not fulfill the promise of a
smart assistant answering a culturally-situated question about \textit{Oktoberfest}.
%
One can find applicable Named Entity modulations by referencing WikiData, a human-interpretable and human-verified representation of Wikipedia.
%
We will want to investigate if this method generates better candidates than an embedding-based approach.
%
We focus on the task of cultural adaptation of entities: given an
entity in English, what is the figuratively similar entity in a target language.
%
For example, the German \textit{Anthony Fauci} is
\textit{Christian Drosten}.
%
An accurate evaluation of this approach requires using Germans and Americans with appropriate cultural backgrounds.
%
This project will show that expert judgment can evaluate a new task in machine translation.


%\subsection{Crowd-Sourcing and Automation}
%\label{sec:crowd}
%
%We define crowd-sourcing and automatic data generation techniques, explain their history, and comment on the repercussions of the wide-spread use of this data pool in \nlp{} today. 
%
%
%Using a nonprofessional user pool is the default manner for collecting large datasets for \nlp{} as it can be done quickly and cheaply.    
%
%Tangential approaches that maximize quantity of data is the 
%scraping of websites and forums, especially Wikipedia, and the creation of synthetic data.  
%%
%Synthetic data is created according to fixed rules or templates. 
%%
%\textit{Augmentation} is a frequent phrasing of this way of creating data~\citep{kafle2017data}.
%%
%This method can create datasets of virtually any scale, but it does not guarantee any authenticity in the data.   
%
%The use of crowd-sourcing for \nlp{} data is a recent phenomenon.  
%%
%ImageNet began the paradigm of large-scale, low-cost data collection when it annotated WordNet images through the use of crowd-sourced workers on Mechanical Turk in 2009~\citep{deng2009imagenet}.  
%%
%This transferred over to natural language processing~\citep{callison2015crowdsourcing}.  
%Large question answering datasets involving Wikipedia and search engines---\squad{}, SearchQA---use crowd-sourcing to generate questions~\citep{rajpurkar-16, dunn-17}.
%
%We note issues in this technique through the lens of two projects: automatic generation of audio data and crowd-sourced generation of questions.  
%%
%In the former, text-to-speech is used to create a dataset of 500,000 audio files.  
%%
%In the latter, Mechanical Turk uses unskilled labor to rewrite sequential questions into a standalone format.  
%
%Chapter~\ref{ch:unspecialized} provides a thorough history of \nlp{} datasets generated or annotated without the use of any specialized users. 
%
%\subsection{Hybrid Data Sources}
%\label{sec:hybrid}
%
%Unlike crowd-sourcing, hybrid approaches aim to oversee unspecialized labor or automatic methods with expert knowledge.
%%
%This combination lowers cost and allows for data scaling, while maintaining a certain level of quality control.  
%%
%We define hybrid user pools and introduce several of the key works in the field.  
%%
%Last, we discuss a project that pairs crowd-sourced workers from Mechanical Turk with expert customer service agents employed by a corporation in Chapter~\ref{ch:semi}. 
%%We introduce our work that pairs customer service professionals with crowd-sourced workers simulating customers.  
%
%For \textit{generation}, crowd-sourced workers can be combined with trained agents to perform a given \nlp{} task.  
%%
%One such example is for creating Wizard-of-Oz personal assistant dialogues~\citep{byrne2019taskmaster}.
%
%For \textit{annotation}, crowd-sourced workers can be supervised by trained experts.
%%
%Clustering errors made by crowd-sourced workers on Named Entity Recognition can identify systemic issues, which theoretically in turn can be escalated to a skilled arbitrator~\citep{nguyen2019explainable}.
%%
%FEVER uses super-annotators on one percent of the data as a comparison point for all other annotations~\citep{thorne2018fever}.
%%
%
%In our work, we pair crowd-sourced workers from Mechanical Turk with actual customer service agents employed by a corporation.
%%
%This set-up is meant to imitate a real customer-service interaction. 
%%
%Customers are impatient and have no incentive to be grammatical, creative, or polite.
%%
%In contrast, customer service agents are beholden to their employers and must handle a plethora of unique customer requests.  
%%
%Having an expert in the conversation allows us to introduce creativity into the conversation, and to have a reliable source of conversation quality.  
%
%\subsection{Expert-Driven Generation and Annotation}
%\label{sec:expert}
%
%
%We define ``experts'', provide a brief summary of relevant datasets, and introduce a dataset \textit{generated} and \textit{annotated} by domain experts.
%
%We formally define ``expert'' as ``a person with a high level of knowledge or skill relating to a particular subject or activity''{Cambridge Dictionary}.  
%%
%In the context of \nlp{} this requires that the person have some sort of incentive to \textit{accurately}, as opposed to quickly, complete their task.  
%%
%These experts can be trained or they can be found in specialized communities of interest.  
%
%Datasets that has stood the test of time generally come from expert, non-noisy sources.  
%%
%Popular datasets come from organizations that have an incentive to control or report their data.
%%
%The Titanic had an accurate list of passengers. 
%%
%The United Nations maintains detailed datasets about global populations.  
%%
%New York City releases the Taxi and Limousine Commission data.
%%
%The World Trade Organization releases a comprehensive collection of legal disputes.  
%
%
%We discuss a project that works with the Diplomacy, a popular board-game, community to generate and annotate a natural conversational dataset for the task of deception in Chapter~\ref{ch:expert}.